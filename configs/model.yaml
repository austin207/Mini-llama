model:
  vocab_size: 8192
  n_layers: 12         # 10â€“13 is safe for 16GB, 12 is a sweet spot
  n_heads: 8           # 8 heads (d_model must be divisible by n_heads)
  d_model: 512         # Embedding dimension (512 is safe for 16GB)
  d_ff: 2048           # Feedforward dimension (4x d_model is standard)
  max_seq_len: 1024     # You can try 1024 if your batch size is small
  dropout: 0.2         # For regularization
