tokenizer:
  vocab_size: 8192
  model_type: bpe
  special_tokens: [] 